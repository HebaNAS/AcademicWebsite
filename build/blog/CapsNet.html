<!DOCTYPE html>
<html lang="en">
	<head>
    <meta charset="utf-8" />
		<link rel="icon" href="../favicon.ico" />
		<meta name="viewport" content="width=device-width" />
		
		<link href="../_app/immutable/assets/0.Bza9aNTp.css" rel="stylesheet">
		<link href="../_app/immutable/assets/3.I0tGCxc4.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.BxcYbPYr.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Bxj4FmGB.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DTNpGt4h.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/C6bp9ilZ.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.S1_i7SzX.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/ae3zVnA3.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.D4tRq8f8.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/3.CE8wL2Qw.js"><title>Heba El-Shimy, FHEA, MBCS | Provost Fellow, Data &amp; AI Specialist</title><!-- HEAD_svelte-1wsgiqp_START --><meta charset="utf-8"><meta name="description" content="This is my personal academic portfolio site and blog where I share what I'm reading or learning. To go to the live website follow this link: https://www.macs.hw.ac.uk/~he4002/"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Heba El-Shimy"><meta name="keywords" content="data, data science, machine learning, AI, artificial intelligence, deep learning, portfolio, research, academic, blog"><meta property="og:site_name" content="Heba El-Shimy Academic Site and Blog"><meta property="og:url" content="https://www.macs.hw.ac.uk/~he4002/"><meta property="og:type" content="website"><meta property="og:title" content="| Data Scientist, Researcher and Lecturer"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta http-equiv="content-language" content="en"><meta http-equiv="twitter-username" content="@heba_el_shimy"><meta http-equiv="title-template" content="%s ∙ Heba El-Shimy"><link rel="canonical" href="https://www.macs.hw.ac.uk/~he4002/"><link rel="alternate" hreflang="x-default" href="https://www.macs.hw.ac.uk/~he4002/"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css" integrity="sha384-ZPe7yZ91iWxYumsBEOn7ieg8q/o+qh/hQpSaPow8T6BwALcXSCS6C6fSRPIAnTQs" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js" integrity="sha384-ljao5I1l+8KYFXG7LNEA7DyaFvuvSCmedUf6Y6JI7LJqiu8q5dEivP2nDdFH31V4" crossorigin="anonymous" data-svelte-h="svelte-gafjoe"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous" onload="renderMathInElement(document.body);" data-svelte-h="svelte-kdnskg"></script><script data-svelte-h="svelte-1m3uih">document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                // customised options
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: true},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
            });
        });
    </script><!-- HEAD_svelte-1wsgiqp_END -->
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents">     <nav class="bg-white dark:bg-theme-action-dark"><div class="mx-auto px-2 sm:px-6 xl:px-8 md:px-6"><div class="relative flex items-center justify-between h-20 xl:h-36 md:h-28"><div class="absolute inset-y-0 right-0 flex items-center md:hidden" data-svelte-h="svelte-1m9ft4i"><div class="inline-block items-center justify-center p-2"><label for="toggleB" class="flex items-center cursor-pointer px-3 py-2"> <div class="relative"> <input type="checkbox" id="toggleB" name="dark-mode" class="sr-only svelte-upvok5">  <div class="block line bg-gray-600 w-14 h-8 rounded-full svelte-upvok5"></div>  <div class="dot absolute left-1 top-1 bg-white w-6 h-6 rounded-full transition svelte-upvok5"></div></div></label></div>  <button type="button" class="btn-burger inline-block items-center justify-center p-2 mobile-menu-button svelte-upvok5" aria-controls="mobile-menu" aria-expanded="false"><span class="sr-only">Open main menu</span> <span class="block-line bg-theme-action dark:bg-theme-fg-light svelte-upvok5"></span> <span class="block-line bg-theme-action dark:bg-theme-fg-light svelte-upvok5"></span> <span class="block-line bg-theme-action dark:bg-theme-fg-light svelte-upvok5"></span></button></div> <div class="flex-1 inline-flex items-center justify-start md:items-stretch md:justify-start"><a href="../" class="inline-flex items-center w-60 md:w-80 mb-4 mt-2"><div class="items-center h-10 pb-2 mb-3"><svg width="65px" height="65px" view-box="0 0 65 65" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns-xlink="http://www.w3.org/1999/xlink" id="butterfly-logo"><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo" transform="translate(2.000000, 1.000000)"><path d="M34.533543,30.7976939 C36.3207547,30.0020964 37.6236897,29.6792453 38.269392,29.6100629 C39.2148847,29.5062893 40.1603774,29.3448637 41.048218,29.0104822 C41.7400419,28.7222222 43.9077568,27.2348008 45.8563941,26.7044025 C51.6792453,23.3951782 59.3354298,15.8197065 56.5104822,9.09748428 C52.9591195,0.645702306 35.2023061,-1.44129979 34.187631,13.6981132 C34.8448637,19.9245283 42.5702306,35.7327044 42.5702306,35.7327044 C42.5702306,35.7327044 46.0293501,41.8553459 55.2767296,40.2180294 C57.4213836,39.7568134 62.1373166,36.8280922 60.730608,32.2389937 C60.730608,32.2389937 60.730608,32.2389937 60.730608,32.2389937 C59.370021,29.3218029 56.5566038,27.2463312 53.3396226,27.0157233 C49.1656184,26.7159329 45.8102725,28.629979 44.4612159,29.2410901 L40.3448637,30.9706499" id="right-wing" stroke="#d99e8e" stroke-width="3"></path><path d="M28.7683438,31.697065 C20.0167715,45.3144654 4.1048218,42.9161426 1.02620545,37.1624738 C-2.0524109,31.408805 2.14465409,14.7589099 16.8343816,21.1928721 C31.524109,27.6268344 34.8333333,44.0807128 30.2442348,51.7714885 C25.6551363,59.4622642 11.4266247,51.990566 19.8207547,43.1121593 C25.5744235,35.8710692 30.9821803,32.3888889 34.5220126,30.7976939" id="left-wing" stroke="#d99e8e" stroke-width="3"></path><path d="M34.2683438,30.2557652 C34.2683438,30.2557652 34.533543,30.7746331 35.0062893,31.6740042 C35.4675052,32.5849057 36.1477987,33.8647799 36.9779874,35.3983229 C37.3930818,36.1708595 37.8427673,36.9895178 38.3155136,37.8888889 L38.4884696,38.2463312 C38.5115304,38.3039832 38.5461216,38.3731656 38.5691824,38.4538784 L38.5922432,38.5230608 L38.6037736,38.5922432 C38.5922432,38.6268344 38.6498952,38.5807128 38.5461216,38.8459119 C38.6844864,38.8920335 38.1194969,39.0649895 38.1886792,39.0303983 L38.1771488,39.0303983 C38.1540881,39.0303983 38.0618449,39.0073375 38.0041929,38.9727463 C37.8542977,38.8574423 37.8888889,38.8689727 37.8658281,38.8459119 C37.6582809,38.5922432 37.5314465,38.3962264 37.3815514,38.1656184 C37.0932914,37.7159329 36.793501,37.2662474 36.5052411,36.8050314 C35.9287212,35.8710692 35.3522013,34.9025157 34.7987421,33.8993711 C34.2222222,32.9077568 33.7033543,31.8584906 33.1614256,30.8322851 C32.6540881,29.7830189 32.1236897,28.745283 31.6740042,27.6960168 C31.2012579,26.6582809 30.7631027,25.6090147 30.3595388,24.5943396 C30.1519916,24.0870021 29.9675052,23.5796646 29.7830189,23.0838574 C29.6907757,22.8186583 29.6100629,22.6111111 29.5293501,22.28826 C29.5293501,22.2651992 29.5178197,22.2536688 29.5178197,22.1844864 C29.5178197,22.1383648 29.5062893,22.172956 29.5178197,22.0691824 C29.5293501,21.9769392 29.5985325,21.8616352 29.6677149,21.8039832 C29.7253669,21.7348008 30.0828092,21.71174 29.932914,21.7348008 C29.9213836,21.7348008 30.0020964,21.7463312 30.0597484,21.7809224 C30.1174004,21.8039832 30.1865828,21.8616352 30.221174,21.8962264 L30.2672956,21.9538784 C30.3134172,22.0230608 30.3480084,22.0807128 30.3825996,22.1383648 L30.567086,22.4842767 C31.0167715,23.3951782 31.408805,24.2484277 31.7777778,25.0440252 C32.5157233,26.6236897 33.1268344,27.9381551 33.5880503,28.8490566 C34.0146751,29.7368973 34.2683438,30.2557652 34.2683438,30.2557652 Z M34.2683438,30.2557652 C34.2683438,30.2557652 34.0031447,29.7368973 33.5303983,28.8375262 C33.2997904,28.3878407 33.0115304,27.8459119 32.6540881,27.2232704 C32.3081761,26.6121593 31.9161426,25.908805 31.466457,25.1708595 C31.0167715,24.4213836 30.5209644,23.6142558 29.9675052,22.7955975 L29.7599581,22.4958071 C29.7253669,22.4496855 29.6907757,22.4035639 29.6561845,22.3689727 L29.6446541,22.3574423 C29.6677149,22.3805031 29.7138365,22.4266247 29.7714885,22.4381551 C29.8291405,22.4612159 29.8983229,22.4496855 29.8637317,22.4496855 C29.7023061,22.4727463 30.048218,22.4266247 30.10587,22.3805031 C30.1750524,22.3343816 30.2442348,22.230608 30.2557652,22.1383648 L30.2557652,22.0691824 C30.2557652,22.0345912 30.2557652,22.057652 30.2557652,22.057652 C30.3249476,22.230608 30.451782,22.4842767 30.567086,22.7033543 C30.8092243,23.1645702 31.0628931,23.6373166 31.3165618,24.1100629 C31.8354298,25.067086 32.3658281,26.0587002 32.9077568,27.0618449 C33.4612159,28.0649895 33.9570231,29.1027254 34.4989518,30.1289308 C34.9947589,31.1781971 35.5366876,32.2044025 36.009434,33.2421384 C36.4937107,34.2798742 36.966457,35.2945493 37.427673,36.2746331 C37.6582809,36.7589099 37.8888889,37.2431866 38.1079665,37.7159329 C38.2232704,37.9350105 38.3501048,38.2002096 38.442348,38.3501048 C38.4308176,38.3501048 38.5,38.384696 38.3616352,38.2924528 C38.3155136,38.269392 38.2348008,38.2578616 38.2232704,38.2578616 L38.2348008,38.2578616 C38.3155136,38.2348008 37.7505241,38.384696 37.8888889,38.4308176 C37.7966457,38.672956 37.8542977,38.615304 37.8427673,38.6268344 C37.8427673,38.6383648 37.8427673,38.6498952 37.8427673,38.6498952 L37.8427673,38.6268344 C37.8312369,38.5807128 37.8197065,38.5230608 37.7966457,38.4769392 L37.6813417,38.1310273 C37.3584906,37.197065 37.0010482,36.3207547 36.6666667,35.5136268 C36.3438155,34.706499 36.009434,33.9800839 35.721174,33.3343816 C35.432914,32.6886792 35.1677149,32.1352201 34.9486373,31.6740042 C34.5220126,30.7746331 34.2683438,30.2557652 34.2683438,30.2557652 Z" id="Shape" fill="#d99e8e" fill-rule="nonzero"></path><path d="M29.8752621,22.0807128 C29.8752621,22.0807128 23.4297694,3.67819706 31.466457,1.26834382" id="antenna-1" stroke="#d99e8e" stroke-width="0.25"></path><path d="M30.2327044,23.0838574 C30.2327044,23.0838574 21.6194969,5.59224319 14.2631027,9.639413" id="antenna-2" stroke="#d99e8e" stroke-width="0.25"></path><circle id="point-1" stroke="#d99e8e" fill="#d99e8e" fill-rule="nonzero" cx="42.4779874" cy="47.5628931" r="1.19916143"></circle><circle id="point-2" stroke="#d99e8e" fill="#d99e8e" fill-rule="nonzero" cx="13.6519916" cy="10.2274633" r="1.19916143"></circle><circle id="point-3" stroke="#d99e8e" fill="#d99e8e" fill-rule="nonzero" cx="32.6540881" cy="1.26834382" r="1.19916143"></circle></g></g></svg></div> <div class="items-center" data-svelte-h="svelte-16lxu98"><span class="inline-block my-name text-theme-action dark:text-theme-fg-light text-2xl mt-3 xl:text-4xl pl-3 font-semibold svelte-upvok5">Heba El-Shimy</span></div></a></div> <div class="flex-1 flex items-center justify-end md:items-stretch md:justify-end main-menu svelte-upvok5"><div class="hidden md:block md:ml-6 md:mt-12"><div class="flex space-x-4"><a href="../" class="text-theme-action dark:text-white px-3 py-2 text-xl font-normal transition-colors ease-in-out duration-500  svelte-upvok5" aria-current="page">Home</a> <a href="../research" class="text-theme-text dark:text-white dark:hover:text-theme-fg hover:text-theme-fg hover:scale-100 px-3 py-2 text-xl font-normal transition-colors ease-in-out duration-500  svelte-upvok5">Research</a> <a href="../posts" class="text-theme-text dark:text-white dark:hover:text-theme-fg hover:text-theme-fg px-3 py-2 text-xl font-normal transition-colors ease-in-out duration-500  svelte-upvok5">Blog</a> <a href="../contact" class="text-theme-text dark:text-white dark:hover:text-theme-fg hover:text-theme-fg px-3 py-2 text-xl font-normal transition-colors ease-in-out duration-500  svelte-upvok5">Contact</a> <div class="flex items-center justify-center w-full mb-12" data-svelte-h="svelte-1jkompv"><label for="toggleA" class="flex items-center cursor-pointer px-3 py-2"> <div class="relative"> <input type="checkbox" id="toggleA" name="dark-mode" class="sr-only svelte-upvok5">  <div class="block line bg-gray-600 w-14 h-8 rounded-full svelte-upvok5"></div>  <div class="dot absolute left-1 top-1 bg-white w-6 h-6 rounded-full transition svelte-upvok5"></div></div></label></div></div></div></div></div></div>  <div class="md:hidden slide svelte-upvok5" id="mobile-menu"><div class="px-2 pt-2 pb-3 space-y-1"><a href="../" class="text-theme-text dark:text-theme-fg-light block px-3 py-2 rounded-md text-base font-normal  svelte-upvok5">Home</a> <a href="../research" class="text-theme-text dark:text-theme-fg-light block px-3 py-2 rounded-md text-base font-normal  svelte-upvok5">Research</a> <a href="../posts" class="text-theme-text dark:text-theme-fg-light block px-3 py-2 rounded-md text-base font-normal  svelte-upvok5">Blog</a> <a href="../contact" class="text-theme-text dark:text-theme-fg-light block px-3 py-2 rounded-md text-base font-normal  svelte-upvok5">Contact</a></div></div> </nav> <div class="custom-container"><article class="w-screen md:w-full lg:w-screen width-adjust h-max bg-theme-fg-dark dark:bg-theme-action-dark grad-1 flex flex-col items-stretch pt-10 pb-20 svelte-8ees3q"><a href="../posts" class="ml-20 underline text-theme-text dark:text-theme-fg-light block px-3 py-2 rounded-md text-base font-normal" data-svelte-h="svelte-dl5ko5">Back to Blog</a> <div class="container w-8/12 mx-auto my-10 py-10 mb-40"><h2 class="font-display font-bold text-3xl">CapsNets, a step-up from CNNs?</h2> <span class="text-md font-normal">5/15/2022</span> <img src="../blogimages/blog-capsule.jpeg" alt="CapsNets, a step-up from CNNs?" class="rounded-md shadow-xl mx-auto my-10 border-bg-white border-8"> <div class="text-xl"><!-- HTML_TAG_START --><p class="text-theme-text dark:text-theme-fg-light">Convolutional Neural Networks (CNNs) have been the state-of-the-art when it comes to computer vision tasks. They are in constant improvement and novel architectures are always coming up that are more accurate while being more effcient and faster. However, according to Geoffery Hinton, the routing mechanism between convolutional layers in CNNs, the pooling mechanism, is the problem as it causes loss of valuable information by discarding pixels with low activation or averaging the values of pixels within a window of predefined dimensions (Sabour, Frosst, and G. E. Hinton, 2017). Hinton et al. proposed a new architecture and named it “Capsule Network” and a new routing mechanism, Dynamic Routing between Capsules, which will be discussed in this blog post.</p>

<br/>
<hr class="border-t-2 border-theme-action shadow-white" width="300" />
<br/>

<h2 class="text-2xl font-display font-bold">What is a Capsule Network?</h2>
<br/>

<p class="text-theme-text dark:text-theme-fg-light">The intuition behind a capsule network is to have a group of neurons that can capture properties of existing entities in an image and performing <em>inverse graphics</em> on them, as shown in Figure 1. In Computer Graphics, a program typically starts with the instantiation parameters of an object (position, size, orientation, deformation, velocity, hue, texture), and uses those to draw the object. Capsule networks (CapsNets) are able to invert this process by learning from objects in input images their instantiation parameters. CapsNets can achieve equivariance, for example, if an entity is present in an image and is rotated by 40 deg, then a capsule network is able to recognise the original entity and that it is rotated. On the other hand, CNNs have a crude approach at equivariance by recognising that the detected entity matches a variant that is transformed (rotated by 40deg). CNNs learn from thousands of original and transformed examples of an entity while on the other hand, CapsNets can learn efficient representations of objects from far fewer examples and simpler networks.</p>
<br/>

<img class="rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8" src="../blogimages/inverse_graphics.png" alt="Inverse Graphics by Capsule Networks">
<div class="text-base text-center">
    <span>Figure 1. Inverse Graphics</span><br/>
    <span>Image Credits: <a class="underline" href="https://hiepph.github.io/post/2018-03-06-capsules/">https://hiepph.github.io/post/2018-03-06-capsules/</a></span>
</div>

<br/>
<br/>

<p class="text-theme-text dark:text-theme-fg-light">Another strength of CapsNets is the dynamic routing algorithm that acts as a disentanglement technique to ”explain-away” part-whole relationships. Lower-layer capsules route by agreement to higher-layer capsules forming <em>parse-tree structure</em>, i.e., the entity in a lower-layer capsule should be part of bigger picture in the higher-layer capsule and should have a similar orientation. An example of a parse tree structure can be seen in Figure 2.</p>
<br/>

<img class="w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8" src="../blogimages/parse_tree.png" alt="Parse Tree Structure by Capsule Networks">
<div class="text-base text-center">
    <span>Figure 2. Example of Parse Tree Structure</span><br/>
    <span>Image Credits: <a class="underline" href="https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f">Towards Data Science</a></span>
</div>

<br/>

<p class="text-theme-text dark:text-theme-fg-light">Dynamic routing in capsule networks can be similar to the purpose of self-attention in transformers in trying to understand part-whole relationships (Vaswani, Shazeer, Parmar, et al., 2017). Transformers employ attention maps to decide which parts of the input to attend to and how information from these different parts contribute to updating the representations. Attention maps in Transformers can be mapped to routing coefficients in CapsNets, the difference is that in Transformers the attention is computed top-down while in CapsNets the routing coefficients are computed bottom-up (Samira Abnar, 2019).</p>
<br/>

<p class="text-theme-text dark:text-theme-fg-light">Dynamic routing algorithm can also identify when one entity in a lower-layer capsule does not match the orientation of the higher-layer capsule and that it is slightly off, as shown in Figure 3. Here, a CNN will detected all entities in the face although some of them do not match the target orientation of the face, nonetheless, the CNN will predict the presence of a pitbull face with a high confidence. CapsNet on the other hand will recognise that the parts. although detected correctly, do not contribute to a whole in a correct way, thus will output a lower activation for a pitbull face capsule.</p>
<br/>

<img class="w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8" src="../blogimages/wrong_face.png" alt="Part-whole relationship by Capsule Networks">
<div class="text-base text-center">
    <span>Figure 3. CapsNets can understand part-whole relationships</span><br/>
    <span>Image Credits: <a class="underline" href="https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f">Towards Data Science</a></span>
</div>

<br/>

<p class="text-theme-text dark:text-theme-fg-light">It is this functionality of dynamic routing that gives CapsNets the advantage of being able to explain the objects they detect in an image by disentangling them, and this was actually achieved in the paper by (Sabour, Frosst, and G. E. Hinton, 2017) on MultiMNIST, where CapsNet was successful in disentangling overlapping digits</p>

<br/>
<br/>

<h2 class="text-2xl font-display font-bold">Capsule Network Architecture</h2>
<br/>

<p class="text-theme-text dark:text-theme-fg-light">Capsule networks consist of two components: an encoder and a decoder. The encoder component is responsible for the network predictions. The first layer in the network is a regular convolutional layer as in any CNN, it takes in images as input and outputs feature maps. In the architecture prposed by (Sabour, Frosst, and G. E. Hinton, 2017), the convolutional layer produces 256 feature maps after convolving a filter of size 9x9 with a stride of 1 across the input which is 28x28 a single-channel image. The output of the first convolutional layer is 256 feature maps of size 20x20 that is followed by a second convolution where the output of 256 6x6 feature maps is reshaped into 32 blocks of size 6x6 and depth of 8, these blocks contain groups of neurons called capsules and are the basic building block for Capsule Networks. The idea behind having these capsules is to learn and represent properties that correspond to a particular entity in an image and learn part-whole relationships as will be described in the following paragraph on how information gets routed from one layer to the next. The output of each capsule is an 8D vector with the encoded properties learnt about an entity, this is comparable to the output of neurons in CNNs which is a single scalar for each neuron. The probability of the existence of an entity in Capsule Networks is represented by the length of the output vector. A squashing function (Eq.1) is used on each capsule to introduce non-linearity by driving the vector length value closer to 0 for short vectors and closer to 1 for long vectors. Squashing is comparable to non-linear activation functions in CNNs that activate only neu- rons with outputs passing certain threshold. The first layer with capsules is called “Primary Capsules” layer.</p>

<br/>

$$
v_j = \frac{||s_j||^2}{1+||s_j||^2} \frac{s_j}{||s_j||} \tag{Eq. 1}
$$

<br/>

<p class="text-theme-text dark:text-theme-fg-light">The second capsule layer is the “Digit Capsules”, as the authors used digit images—the MNIST dataset (LeCun and Cortes, 2010)—as input to classify digits, but the second layer can more generally be named as <em>Class Capsules</em>. This layer is responsible for the prediction and it is expected to have a number of capsules equalling that of the classes in the dataset. Routing between Primary Capsules and Digit Capsules is what Hinton named “Dynamic Routing” where lower layer capsules choose which higher layer capsules to send their information to. The algorithm for the dynamic routing algorithm is shown in Algorithm 1 below. Mainly, lower layer capsules try to predict the output of every higher layer capsule by using a transformation matrix \(W_{ij}\) consisting of weights that are initialised randomly at the start of the training process and are updated through backpropagation. By multiplying the weights in the transformation matrix with the lower layer capsule output, we get the predicted output of higher layer capsules (Eq. 2). The predicted output is used to calculate routing coefficients cij which are basically how likely a lower layer capsule will connect with a higher layer one. Routing coefficients of a certain layer should sum up to 1, hence \(c_{ij}\) is calculated as a softmax of log prior probabilities \(b_{ij}\) (Eq. 3). Log priors \(b_{ij}\) are initialised to zeros and are learnt over several iterations discriminatively along with other weights in the network and is dependent on the capsules types and locations. They are refined by adding a scalar value representing the agreement between two capsules, which can be calculated as a similarity score between the predicted outputs and the activity vector of a higher layer capsule (Eq. 4).</p>

<br/>

$$
\hat{u}_{j|i} = W_{ij}u_i \tag{Eq. 2}
$$

<br/>

$$
c_{ij} = \frac{exp(b_{ij})}{\sum_k{exp(b_{ik})}} \tag{Eq. 3}
$$

<br/>

$$
b_{ij} = b_{ij} + \hat{u}_{j|i}.{v_j} \tag{Eq. 4}
$$

<br/>
<br/>

<img class="w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8" src="../blogimages/algorithm.png" alt="Dynamic Routing Algorithm">
<div class="text-base text-center">
    <span>Algorithm 1. Dynamic Routing</span><br/>
</div>

<br/>

<p class="text-theme-text dark:text-theme-fg-light">The network also contains a decoder part where it takes the output vector of the detected class from the encoder (masking other capsules) and feeds that to a series of fully connected layers to try and reconstruct the original image. The full architecture of a Capsule Network is shown in Figure 4.</p>
<br/>

<p class="text-theme-text dark:text-theme-fg-light">The loss of the network is over two stages: firstly, for the encoder and is called the ”Margin Loss”, and secondly, for the decoder and is called “Reconstruction Loss”. The margin Loss (Eq .5) is calculated for each output capsule and aims to allow a certain capsule of class k to have a long instantiation vector if and only if the entities of that class exist in the image; in the paper by (Sabour, Frosst, and G. E. Hinton, 2017) this translates to capsule of class \(k\) having a long instantiation vector if and only if digit \(k\) exists in the image. The total margin loss is the sum of all digit capsules losses. The reconstruction loss is calculated using mean squared error (MSE) between the reconstructed image and the original. The total loss of a Capsule Network is the sum of margin and reconstruction losses adding a down-weighing factor for the decoder loss (0.0005) so it does not dominate the margin loss.</p>

<br/>

$$
L_k = T_k max(0, m^+ - ||v_k||)^2 + \lambda(1 - T_k) max(0, ||v_k|| - m^-)^2 \tag{Eq. 5}
$$
where \(T_k\) = 1, \(m^+\) = 0.9, \(m^-\) = 0.1, \(\lambda\) = 0.5 is a weighing factor to minimise the effect of loss of absent digits from shrinking the length of other digits’ activity vectors.

<br/>
<img class="w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8" src="../blogimages/capsnet.png" alt="CapsNet Architecture">
<div class="text-base text-center">
    <span>Figure 4. Capsule Network Architecture as in (Sabour, Frosst, and G. E. Hinton, 2017)</span><br/>
</div>

<br/>
<br/>

<h2 class="text-2xl font-display font-bold">Limitations of Capsule Networks</h2>
<br/>

<p class="text-theme-text dark:text-theme-fg-light">As any DL technique, CapsNets have several limitations that are reported in the literature and also confirmed in our experiments. The main limitation of CapsNets is that they are very sensitive to noise in the input images, as they tend to model everything in an image. This affects the network’s performance drastically, especially during the reconstruction phase. CapsNet’s performance is state-of-the-art on simple datasets such as MNIST (LeCun and Cortes, 2010) and FashionMNIST (Xiao, Li, J. Wang, and Huo, 2017), but is affected and becomes short of state-of-the-art performance on more complex datasets such as CIFAR10 (Krizhevsky, V. Nair, and G. Hinton, n.d.) and SVHN (Netzer, T. Wang, Coates, et al., 2011) which are characterised by high intra-class variation and much noise and background scenes.</p>
<br/>
<p class="text-theme-text dark:text-theme-fg-light">CapsNets are also computationally expensive due to the extremely large number of trainable parameters that is aggravated with the use of high resolution images. There have been some approaches in the literature that were able to achieve similar performance of the original CapsNets but with only 2% of the parameters (Mazzia, Salvetti, and Chiaberge, 2021).
The dynamic routing algorithm is found to be slow in training and convergence. Additionally, the number of routing iterations seems to be the most important hyperparameter resulting in significant variation in performance (Kwabena Patrick, Felix Adekoya, Abra Mighty, and Edward, 2019). There are several approaches in the literature to modify the original dynamic routing algorithm or replace it altogether. One suggestion by (Ramasinghe, Athuraliya, and Khan, 2018) was to include the routing coefficients with other learnable parameters in the network that get updated with backpropagation. Another approach is FastCapsNet (Mobiny and Van Nguyen, 2018) where the authors created a novel consistent dynamic routing algorithm, where they enforced all capsules in the Primary Capsules layer that are in a specific pixel area to have the same routing coefficients which resulted in much faster learning and better handling of high-dimensional data. A second approach by (Peer, Stabinger, and Rogriguez-Sanchez, 2018) developed a new routing algorithm that allows the training of deeper CapsNets. They argued that the original dynamic routing algorithm does not guarantee invariance or equivariance as the ”agreement” between capsules can be influenced by a large weight matrix \(W_{ij}\), thus they replaced the calculation of \(b_{ij}\) as the dot product of the prediction vector \(\hat{u}_{j|i}\) and the activity vector \(v_j\) with the negative euclidean distance, so that a large weight matrix does not influence agreement by automatically increasing its value.</p>
<br/>
<p class="text-theme-text dark:text-theme-fg-light">While above limitations might sound like Capsule Networks cannot replace or be on-par with CNNs, however, we agree with (Sabour, Frosst, and G. E. Hinton, 2017) that CapsNets as a field of research is still very new and could be compared to CNNs in the late 1990s with Lecun’s CNN architecture (Lecun, Bottou, Bengio, and Ha↵ner, 1998). Since then, CNNs have come a long way, with many advances in the architecture that allowed them to be state-of-the-art in solving many computer vision problems and beyond. Capsule Networks as a field is in need of more research and innovation in the best use cases and architectures to prove their capabilities. And indeed it has been the case for the past two to three years were many researchers are working on improving CapsNets. As (Mazzia, Salvetti, and Chiaberge, 2021) stated in their paper, CapsNets have proved after extensive experimentation that they are capable of efficiently embedding visual representations and are able to generalise better on new data than CNNs do.</p>

<br/>
<br/>
<hr class="border-t-2 border-theme-action shadow-white" width="300" />
<br/>
<br/>

<h2 class="text-xl font-display font-bold">References</h2>
<br/>

<ol class="text-base list-decimal">
    <li>Krizhevsky, Alex, Vinod Nair, and Geofferey Hinton. “CIFAR-10”. Available at: <a class="underline" href="http://www.cs.toronto.edu/~kriz/cifar.html">http://www.cs.toronto.edu/~kriz/cifar.html</a></li>
    <br/>
    <li>Kwabena Patrick, Mensah, Adebayo Felix Adekoya, Ayidzoe Abra Mighty, and Baagyire Y. Edward (2019). “Capsule Networks – A survey”. In: issn: 22131248. doi: 10.1016/j.jksuci.2019.09.014</li>
    <br/>
    <li>Lecun, Y., L. Bottou, Y. Bengio, and P. Ha↵ner (1998). “Gradient-based learning applied to document recog- nition”. In: <em>Proceedings of the IEEE 86.11</em>. Available at: <a class="underline" href="https://ieeexplore.ieee.org/document/726791">https://ieeexplore.ieee.org/document/726791</a>, pp. 2278–2324. issn: 1558-2256. doi: 10.1109/5.726791</li>
    <br/>
    <li>LeCun, Yann and Corinna Cortes (2010). “MNIST handwritten digit database”. Available at: <a class="underline" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></li>
    <br/>
    <li>Mazzia, Vittorio, Francesco Salvetti, and Marcello Chiaberge (Jan. 2021). “Efficient-CapsNet: Capsule Network with Self-Attention Routing”. In: arXiv: 2101.12491. Available at: <a class="underline" href="http://arxiv.org/abs/2101.12491">http://arxiv.org/abs/2101.12491</a></li>
    <br/>
    <li>Mobiny, Aryan and Hien Van Nguyen (2018). “Fast CapsNet for Lung Cancer Screening”. In: <em>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018</em>. pp. 741–749. isbn: 978-3-030-00934-2</li>
    <br/>
    <li>Netzer, Y., T. Wang, A. Coates, A. Bissacco, B. Wu, and A.Y. Ng (2011). “Reading digits in natural images with unsupervised feature learning”. In: <em>NIPS Workshop on Deep Learning and Unsupervised Feature Learning</em></li>
    <br/>
    <li>Peer, David, Sebastian Stabinger, and Antonio Rogriguez-Sanchez (Dec. 2018). “Training Deep Capsule Networks”</li>
    <br/>
    <li>Ramasinghe, S., C.D. Athuraliya, and S.H. Khan (2018). “A Context-aware Capsule Network for Multi-label Classification”. In: arXiv 1810.06231v2. pp. 1–9</li>
    <br/>
    <li>Sabour, Sara, Nicholas Frosst, and Geo↵rey E Hinton (2017). “Dynamic Routing Between Capsules”. In: arXiv: 1710.09829</li>
    <br/>
    <li>Samira Abnar (Mar. 2019). From Attention in Transformers to Dynamic Routing in Capsule Nets. Available at: <a class="underline" href="https://samiraabnar.github.io/articles/2019-03/capsule">https://samiraabnar.github.io/articles/2019-03/capsule</a></li>
    <br/>
    <li>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin (June 2017). “Attention Is All You Need”. In: <em>Advances in Neural Information Processing Systems 2017</em>, pp. 5999–6009. issn: 10495258. arXiv: 1706.03762. Available at: <a class="underline" href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a></li>
    <br/>
    <li>Xiao, Hongze, Liqing Li, Jun Wang, and Shuhuai Huo (2017). “The application of improved threshold segmentation on detection of color fluff”. In: <em>2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)</em>, pp. 1–6. doi: 10.1109/CISP-BMEI.2017.8302073</li>
</ol>
<!-- HTML_TAG_END --></div></div> </article></div> <footer class="bg-white dark:bg-theme-action-dark h-20 xl:h-36 md:h-28 p-5 flex items-center absolute inset-x-0 bottom-0">© 2025 by Heba El-Shimy
</footer> 
			
			<script>
				{
					__sveltekit_jdclbz = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/~he4002"
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.BxcYbPYr.js"),
						import("../_app/immutable/entry/app.S1_i7SzX.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data: [null,{"type":"data","data":{post:{path:"src/posts/CapsNet.md",attributes:{title:"CapsNets, a step-up from CNNs?",date:new Date(1652572800000),excerpt:"A gentle intro to Capsule Networks, their architecture and potential.",thumbnail:"blogimages/blog-capsule.jpeg"},html:"\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">Convolutional Neural Networks (CNNs) have been the state-of-the-art when it comes to computer vision tasks. They are in constant improvement and novel architectures are always coming up that are more accurate while being more effcient and faster. However, according to Geoffery Hinton, the routing mechanism between convolutional layers in CNNs, the pooling mechanism, is the problem as it causes loss of valuable information by discarding pixels with low activation or averaging the values of pixels within a window of predefined dimensions (Sabour, Frosst, and G. E. Hinton, 2017). Hinton et al. proposed a new architecture and named it “Capsule Network” and a new routing mechanism, Dynamic Routing between Capsules, which will be discussed in this blog post.\u003C/p>\n\n\u003Cbr/>\n\u003Chr class=\"border-t-2 border-theme-action shadow-white\" width=\"300\" />\n\u003Cbr/>\n\n\u003Ch2 class=\"text-2xl font-display font-bold\">What is a Capsule Network?\u003C/h2>\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">The intuition behind a capsule network is to have a group of neurons that can capture properties of existing entities in an image and performing \u003Cem>inverse graphics\u003C/em> on them, as shown in Figure 1. In Computer Graphics, a program typically starts with the instantiation parameters of an object (position, size, orientation, deformation, velocity, hue, texture), and uses those to draw the object. Capsule networks (CapsNets) are able to invert this process by learning from objects in input images their instantiation parameters. CapsNets can achieve equivariance, for example, if an entity is present in an image and is rotated by 40 deg, then a capsule network is able to recognise the original entity and that it is rotated. On the other hand, CNNs have a crude approach at equivariance by recognising that the detected entity matches a variant that is transformed (rotated by 40deg). CNNs learn from thousands of original and transformed examples of an entity while on the other hand, CapsNets can learn efficient representations of objects from far fewer examples and simpler networks.\u003C/p>\n\u003Cbr/>\n\n\u003Cimg class=\"rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8\" src=\"../blogimages/inverse_graphics.png\" alt=\"Inverse Graphics by Capsule Networks\">\n\u003Cdiv class=\"text-base text-center\">\n    \u003Cspan>Figure 1. Inverse Graphics\u003C/span>\u003Cbr/>\n    \u003Cspan>Image Credits: \u003Ca class=\"underline\" href=\"https://hiepph.github.io/post/2018-03-06-capsules/\">https://hiepph.github.io/post/2018-03-06-capsules/\u003C/a>\u003C/span>\n\u003C/div>\n\n\u003Cbr/>\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">Another strength of CapsNets is the dynamic routing algorithm that acts as a disentanglement technique to ”explain-away” part-whole relationships. Lower-layer capsules route by agreement to higher-layer capsules forming \u003Cem>parse-tree structure\u003C/em>, i.e., the entity in a lower-layer capsule should be part of bigger picture in the higher-layer capsule and should have a similar orientation. An example of a parse tree structure can be seen in Figure 2.\u003C/p>\n\u003Cbr/>\n\n\u003Cimg class=\"w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8\" src=\"../blogimages/parse_tree.png\" alt=\"Parse Tree Structure by Capsule Networks\">\n\u003Cdiv class=\"text-base text-center\">\n    \u003Cspan>Figure 2. Example of Parse Tree Structure\u003C/span>\u003Cbr/>\n    \u003Cspan>Image Credits: \u003Ca class=\"underline\" href=\"https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f\">Towards Data Science\u003C/a>\u003C/span>\n\u003C/div>\n\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">Dynamic routing in capsule networks can be similar to the purpose of self-attention in transformers in trying to understand part-whole relationships (Vaswani, Shazeer, Parmar, et al., 2017). Transformers employ attention maps to decide which parts of the input to attend to and how information from these different parts contribute to updating the representations. Attention maps in Transformers can be mapped to routing coefficients in CapsNets, the difference is that in Transformers the attention is computed top-down while in CapsNets the routing coefficients are computed bottom-up (Samira Abnar, 2019).\u003C/p>\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">Dynamic routing algorithm can also identify when one entity in a lower-layer capsule does not match the orientation of the higher-layer capsule and that it is slightly off, as shown in Figure 3. Here, a CNN will detected all entities in the face although some of them do not match the target orientation of the face, nonetheless, the CNN will predict the presence of a pitbull face with a high confidence. CapsNet on the other hand will recognise that the parts. although detected correctly, do not contribute to a whole in a correct way, thus will output a lower activation for a pitbull face capsule.\u003C/p>\n\u003Cbr/>\n\n\u003Cimg class=\"w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8\" src=\"../blogimages/wrong_face.png\" alt=\"Part-whole relationship by Capsule Networks\">\n\u003Cdiv class=\"text-base text-center\">\n    \u003Cspan>Figure 3. CapsNets can understand part-whole relationships\u003C/span>\u003Cbr/>\n    \u003Cspan>Image Credits: \u003Ca class=\"underline\" href=\"https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f\">Towards Data Science\u003C/a>\u003C/span>\n\u003C/div>\n\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">It is this functionality of dynamic routing that gives CapsNets the advantage of being able to explain the objects they detect in an image by disentangling them, and this was actually achieved in the paper by (Sabour, Frosst, and G. E. Hinton, 2017) on MultiMNIST, where CapsNet was successful in disentangling overlapping digits\u003C/p>\n\n\u003Cbr/>\n\u003Cbr/>\n\n\u003Ch2 class=\"text-2xl font-display font-bold\">Capsule Network Architecture\u003C/h2>\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">Capsule networks consist of two components: an encoder and a decoder. The encoder component is responsible for the network predictions. The first layer in the network is a regular convolutional layer as in any CNN, it takes in images as input and outputs feature maps. In the architecture prposed by (Sabour, Frosst, and G. E. Hinton, 2017), the convolutional layer produces 256 feature maps after convolving a filter of size 9x9 with a stride of 1 across the input which is 28x28 a single-channel image. The output of the first convolutional layer is 256 feature maps of size 20x20 that is followed by a second convolution where the output of 256 6x6 feature maps is reshaped into 32 blocks of size 6x6 and depth of 8, these blocks contain groups of neurons called capsules and are the basic building block for Capsule Networks. The idea behind having these capsules is to learn and represent properties that correspond to a particular entity in an image and learn part-whole relationships as will be described in the following paragraph on how information gets routed from one layer to the next. The output of each capsule is an 8D vector with the encoded properties learnt about an entity, this is comparable to the output of neurons in CNNs which is a single scalar for each neuron. The probability of the existence of an entity in Capsule Networks is represented by the length of the output vector. A squashing function (Eq.1) is used on each capsule to introduce non-linearity by driving the vector length value closer to 0 for short vectors and closer to 1 for long vectors. Squashing is comparable to non-linear activation functions in CNNs that activate only neu- rons with outputs passing certain threshold. The first layer with capsules is called “Primary Capsules” layer.\u003C/p>\n\n\u003Cbr/>\n\n$$\nv_j = \\frac{||s_j||^2}{1+||s_j||^2} \\frac{s_j}{||s_j||} \\tag{Eq. 1}\n$$\n\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">The second capsule layer is the “Digit Capsules”, as the authors used digit images—the MNIST dataset (LeCun and Cortes, 2010)—as input to classify digits, but the second layer can more generally be named as \u003Cem>Class Capsules\u003C/em>. This layer is responsible for the prediction and it is expected to have a number of capsules equalling that of the classes in the dataset. Routing between Primary Capsules and Digit Capsules is what Hinton named “Dynamic Routing” where lower layer capsules choose which higher layer capsules to send their information to. The algorithm for the dynamic routing algorithm is shown in Algorithm 1 below. Mainly, lower layer capsules try to predict the output of every higher layer capsule by using a transformation matrix \\(W_{ij}\\) consisting of weights that are initialised randomly at the start of the training process and are updated through backpropagation. By multiplying the weights in the transformation matrix with the lower layer capsule output, we get the predicted output of higher layer capsules (Eq. 2). The predicted output is used to calculate routing coefficients cij which are basically how likely a lower layer capsule will connect with a higher layer one. Routing coefficients of a certain layer should sum up to 1, hence \\(c_{ij}\\) is calculated as a softmax of log prior probabilities \\(b_{ij}\\) (Eq. 3). Log priors \\(b_{ij}\\) are initialised to zeros and are learnt over several iterations discriminatively along with other weights in the network and is dependent on the capsules types and locations. They are refined by adding a scalar value representing the agreement between two capsules, which can be calculated as a similarity score between the predicted outputs and the activity vector of a higher layer capsule (Eq. 4).\u003C/p>\n\n\u003Cbr/>\n\n$$\n\\hat{u}_{j|i} = W_{ij}u_i \\tag{Eq. 2}\n$$\n\n\u003Cbr/>\n\n$$\nc_{ij} = \\frac{exp(b_{ij})}{\\sum_k{exp(b_{ik})}} \\tag{Eq. 3}\n$$\n\n\u003Cbr/>\n\n$$\nb_{ij} = b_{ij} + \\hat{u}_{j|i}.{v_j} \\tag{Eq. 4}\n$$\n\n\u003Cbr/>\n\u003Cbr/>\n\n\u003Cimg class=\"w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8\" src=\"../blogimages/algorithm.png\" alt=\"Dynamic Routing Algorithm\">\n\u003Cdiv class=\"text-base text-center\">\n    \u003Cspan>Algorithm 1. Dynamic Routing\u003C/span>\u003Cbr/>\n\u003C/div>\n\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">The network also contains a decoder part where it takes the output vector of the detected class from the encoder (masking other capsules) and feeds that to a series of fully connected layers to try and reconstruct the original image. The full architecture of a Capsule Network is shown in Figure 4.\u003C/p>\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">The loss of the network is over two stages: firstly, for the encoder and is called the ”Margin Loss”, and secondly, for the decoder and is called “Reconstruction Loss”. The margin Loss (Eq .5) is calculated for each output capsule and aims to allow a certain capsule of class k to have a long instantiation vector if and only if the entities of that class exist in the image; in the paper by (Sabour, Frosst, and G. E. Hinton, 2017) this translates to capsule of class \\(k\\) having a long instantiation vector if and only if digit \\(k\\) exists in the image. The total margin loss is the sum of all digit capsules losses. The reconstruction loss is calculated using mean squared error (MSE) between the reconstructed image and the original. The total loss of a Capsule Network is the sum of margin and reconstruction losses adding a down-weighing factor for the decoder loss (0.0005) so it does not dominate the margin loss.\u003C/p>\n\n\u003Cbr/>\n\n$$\nL_k = T_k max(0, m^+ - ||v_k||)^2 + \\lambda(1 - T_k) max(0, ||v_k|| - m^-)^2 \\tag{Eq. 5}\n$$\nwhere \\(T_k\\) = 1, \\(m^+\\) = 0.9, \\(m^-\\) = 0.1, \\(\\lambda\\) = 0.5 is a weighing factor to minimise the effect of loss of absent digits from shrinking the length of other digits’ activity vectors.\n\n\u003Cbr/>\n\u003Cimg class=\"w-16 rounded-md shadow-xl mx-auto mt-10 mb-4 border-bg-fg-dark border-8\" src=\"../blogimages/capsnet.png\" alt=\"CapsNet Architecture\">\n\u003Cdiv class=\"text-base text-center\">\n    \u003Cspan>Figure 4. Capsule Network Architecture as in (Sabour, Frosst, and G. E. Hinton, 2017)\u003C/span>\u003Cbr/>\n\u003C/div>\n\n\u003Cbr/>\n\u003Cbr/>\n\n\u003Ch2 class=\"text-2xl font-display font-bold\">Limitations of Capsule Networks\u003C/h2>\n\u003Cbr/>\n\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">As any DL technique, CapsNets have several limitations that are reported in the literature and also confirmed in our experiments. The main limitation of CapsNets is that they are very sensitive to noise in the input images, as they tend to model everything in an image. This affects the network’s performance drastically, especially during the reconstruction phase. CapsNet’s performance is state-of-the-art on simple datasets such as MNIST (LeCun and Cortes, 2010) and FashionMNIST (Xiao, Li, J. Wang, and Huo, 2017), but is affected and becomes short of state-of-the-art performance on more complex datasets such as CIFAR10 (Krizhevsky, V. Nair, and G. Hinton, n.d.) and SVHN (Netzer, T. Wang, Coates, et al., 2011) which are characterised by high intra-class variation and much noise and background scenes.\u003C/p>\n\u003Cbr/>\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">CapsNets are also computationally expensive due to the extremely large number of trainable parameters that is aggravated with the use of high resolution images. There have been some approaches in the literature that were able to achieve similar performance of the original CapsNets but with only 2% of the parameters (Mazzia, Salvetti, and Chiaberge, 2021).\nThe dynamic routing algorithm is found to be slow in training and convergence. Additionally, the number of routing iterations seems to be the most important hyperparameter resulting in significant variation in performance (Kwabena Patrick, Felix Adekoya, Abra Mighty, and Edward, 2019). There are several approaches in the literature to modify the original dynamic routing algorithm or replace it altogether. One suggestion by (Ramasinghe, Athuraliya, and Khan, 2018) was to include the routing coefficients with other learnable parameters in the network that get updated with backpropagation. Another approach is FastCapsNet (Mobiny and Van Nguyen, 2018) where the authors created a novel consistent dynamic routing algorithm, where they enforced all capsules in the Primary Capsules layer that are in a specific pixel area to have the same routing coefficients which resulted in much faster learning and better handling of high-dimensional data. A second approach by (Peer, Stabinger, and Rogriguez-Sanchez, 2018) developed a new routing algorithm that allows the training of deeper CapsNets. They argued that the original dynamic routing algorithm does not guarantee invariance or equivariance as the ”agreement” between capsules can be influenced by a large weight matrix \\(W_{ij}\\), thus they replaced the calculation of \\(b_{ij}\\) as the dot product of the prediction vector \\(\\hat{u}_{j|i}\\) and the activity vector \\(v_j\\) with the negative euclidean distance, so that a large weight matrix does not influence agreement by automatically increasing its value.\u003C/p>\n\u003Cbr/>\n\u003Cp class=\"text-theme-text dark:text-theme-fg-light\">While above limitations might sound like Capsule Networks cannot replace or be on-par with CNNs, however, we agree with (Sabour, Frosst, and G. E. Hinton, 2017) that CapsNets as a field of research is still very new and could be compared to CNNs in the late 1990s with Lecun’s CNN architecture (Lecun, Bottou, Bengio, and Ha↵ner, 1998). Since then, CNNs have come a long way, with many advances in the architecture that allowed them to be state-of-the-art in solving many computer vision problems and beyond. Capsule Networks as a field is in need of more research and innovation in the best use cases and architectures to prove their capabilities. And indeed it has been the case for the past two to three years were many researchers are working on improving CapsNets. As (Mazzia, Salvetti, and Chiaberge, 2021) stated in their paper, CapsNets have proved after extensive experimentation that they are capable of efficiently embedding visual representations and are able to generalise better on new data than CNNs do.\u003C/p>\n\n\u003Cbr/>\n\u003Cbr/>\n\u003Chr class=\"border-t-2 border-theme-action shadow-white\" width=\"300\" />\n\u003Cbr/>\n\u003Cbr/>\n\n\u003Ch2 class=\"text-xl font-display font-bold\">References\u003C/h2>\n\u003Cbr/>\n\n\u003Col class=\"text-base list-decimal\">\n    \u003Cli>Krizhevsky, Alex, Vinod Nair, and Geofferey Hinton. “CIFAR-10”. Available at: \u003Ca class=\"underline\" href=\"http://www.cs.toronto.edu/~kriz/cifar.html\">http://www.cs.toronto.edu/~kriz/cifar.html\u003C/a>\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Kwabena Patrick, Mensah, Adebayo Felix Adekoya, Ayidzoe Abra Mighty, and Baagyire Y. Edward (2019). “Capsule Networks – A survey”. In: issn: 22131248. doi: 10.1016/j.jksuci.2019.09.014\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Lecun, Y., L. Bottou, Y. Bengio, and P. Ha↵ner (1998). “Gradient-based learning applied to document recog- nition”. In: \u003Cem>Proceedings of the IEEE 86.11\u003C/em>. Available at: \u003Ca class=\"underline\" href=\"https://ieeexplore.ieee.org/document/726791\">https://ieeexplore.ieee.org/document/726791\u003C/a>, pp. 2278–2324. issn: 1558-2256. doi: 10.1109/5.726791\u003C/li>\n    \u003Cbr/>\n    \u003Cli>LeCun, Yann and Corinna Cortes (2010). “MNIST handwritten digit database”. Available at: \u003Ca class=\"underline\" href=\"http://yann.lecun.com/exdb/mnist/\">http://yann.lecun.com/exdb/mnist/\u003C/a>\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Mazzia, Vittorio, Francesco Salvetti, and Marcello Chiaberge (Jan. 2021). “Efficient-CapsNet: Capsule Network with Self-Attention Routing”. In: arXiv: 2101.12491. Available at: \u003Ca class=\"underline\" href=\"http://arxiv.org/abs/2101.12491\">http://arxiv.org/abs/2101.12491\u003C/a>\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Mobiny, Aryan and Hien Van Nguyen (2018). “Fast CapsNet for Lung Cancer Screening”. In: \u003Cem>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018\u003C/em>. pp. 741–749. isbn: 978-3-030-00934-2\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Netzer, Y., T. Wang, A. Coates, A. Bissacco, B. Wu, and A.Y. Ng (2011). “Reading digits in natural images with unsupervised feature learning”. In: \u003Cem>NIPS Workshop on Deep Learning and Unsupervised Feature Learning\u003C/em>\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Peer, David, Sebastian Stabinger, and Antonio Rogriguez-Sanchez (Dec. 2018). “Training Deep Capsule Networks”\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Ramasinghe, S., C.D. Athuraliya, and S.H. Khan (2018). “A Context-aware Capsule Network for Multi-label Classification”. In: arXiv 1810.06231v2. pp. 1–9\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Sabour, Sara, Nicholas Frosst, and Geo↵rey E Hinton (2017). “Dynamic Routing Between Capsules”. In: arXiv: 1710.09829\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Samira Abnar (Mar. 2019). From Attention in Transformers to Dynamic Routing in Capsule Nets. Available at: \u003Ca class=\"underline\" href=\"https://samiraabnar.github.io/articles/2019-03/capsule\">https://samiraabnar.github.io/articles/2019-03/capsule\u003C/a>\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin (June 2017). “Attention Is All You Need”. In: \u003Cem>Advances in Neural Information Processing Systems 2017\u003C/em>, pp. 5999–6009. issn: 10495258. arXiv: 1706.03762. Available at: \u003Ca class=\"underline\" href=\"http://arxiv.org/abs/1706.03762\">http://arxiv.org/abs/1706.03762\u003C/a>\u003C/li>\n    \u003Cbr/>\n    \u003Cli>Xiao, Hongze, Liqing Li, Jun Wang, and Shuhuai Huo (2017). “The application of improved threshold segmentation on detection of color fluff”. In: \u003Cem>2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)\u003C/em>, pp. 1–6. doi: 10.1109/CISP-BMEI.2017.8302073\u003C/li>\n\u003C/ol>\n"}},"uses":{"params":["slug"]}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
